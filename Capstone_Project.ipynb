{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**: The aim is to create an application that can identify and differentiate abusive and hate content present in textual manner (includes emojis)\n",
        "\n",
        "Steps followed to achieve the same:\n",
        "1. **Data collection:** Data was collected from different social media sources - Twitter, Reddit, Instagram etc and also from famous data sources like kaggle. The famous hate base corpus was also used and a few other publicly available datasets were combined to constitute the final dataset.\n",
        "\n",
        "  Data collection from Twitter using Tweepy API: https://colab.research.google.com/drive/1SEaDBry6jDTKgMUi5FA8ivwq4Qs-SIpI\n",
        "  \n",
        "  Data collection from Facebook using FacebookGraphAPI: https://drive.google.com/file/d/12g3YshwpgHB3qgbBHrMyt6Es5JmtF2Bh/view?usp=drive_link\n",
        "  \n",
        "  Data collection from Reddit and other online sources using web scraping : https://colab.research.google.com/drive/1Cn-sOu7VoghugfFSZKk-L4uMa2iT5Qxm\n",
        "  \n",
        "  Hate Speech Dataset from Hugging Face : https://huggingface.co/datasets/hate_speech_offensive\n",
        "  \n",
        "  Other small datasets were collected in csv and excel formats, hence their links aren't available.\n",
        "  \n",
        "2. **Data Merging:** The data obtained from different sources were first converted to a standard and uniform format (i.e. csv) since the various datasets obtained varied in format (excel, csv, text, etc).\n",
        "\n",
        "  The datasets which were previously labelled were made to lose their labels while being merged since most of the data was unlabelled. Moreover, the datasets which were labelled did not have the same labels across them (for example, one labelled dataset labelled the dataset as offensive or hate speech whereas the other as degree of hate). Hence to maintain uniformity, only the text columns was preserved from all the datasets and the labels were dropped.\n",
        "  \n",
        "  The final merged dataset: https://docs.google.com/spreadsheets/d/1to6G_JmMbQhDFlfXOUiWWsivVapLVNy7/edit?usp=sharing&ouid=107879742704535972256&rtpof=true&sd=true\n",
        "\n",
        "3. **Data cleaning**: https://colab.research.google.com/drive/1B0XHgVp65mlze1MIY4PZe0stlZ5LsfqQ#scrollTo=9kKH-BfxDYBT\n",
        "\n",
        "4. **Data presentation**: https://colab.research.google.com/drive/1B0XHgVp65mlze1MIY4PZe0stlZ5LsfqQ#scrollTo=BW0MjZyhfmFf\n",
        "5. **Data preprocessing** (Emoji Analysis + Emoji & Emoticons replacement, Spell Check, Tranliteration, Translation, General Text Processing)\n",
        "\n",
        "  Code: https://colab.research.google.com/drive/1B0XHgVp65mlze1MIY4PZe0stlZ5LsfqQ#scrollTo=9kKH-BfxDYBT  \n",
        "\n",
        "6. **Sentiment analysis and Assigning polarity to text**: https://colab.research.google.com/drive/1o8Aey1dip-9rBTPVjhdil13yaJZmPbxM\n",
        "\n",
        "7. **Data labelling:** Rudimentary tests were done using labelled, semi-labelled and unlabelled data using a subset of the dataset obtained and labelling it manually. In the analysis it was found that the models performed best when the dataset was labelled. Hence, it was decided to label the entire dataset and then train the models.\n",
        "Since there were over 1 lakh data points, it was not practically feasible to manually label all of them, hence an API called Parallel Dots API was used to label them.\n",
        "\n",
        "  The code for the same: https://colab.research.google.com/drive/1S62JYU3eaYsR2HrQGeflKe3DcqWn3IaP#scrollTo=PT1nb19jMod5\n",
        "  Note: This was done in batches since the API has a restriction on the number of rows it can handle at a time. Hence, the dataset was split and then labelled separately.\n",
        "  After labelling the datasets, they were finally merged into one single dataset again. This time it was a labelled dataset with percentages of hate speech, abusive and none.\n",
        "\n",
        "  Link for merging of the datasets: https://colab.research.google.com/drive/1Jc3_Ilt12EL-ZLQpAfX2dd_qL2iIuqm5\n",
        "\n",
        "8. **Data Split and Training**:\n",
        "The data was split in a 70:20:10 ratio for training, testing and validation and was then used to train 5 non-deep learning models (Random Forest, KNN, Naive Bayes, Decision Tree and SVM) and two deep learning models (Bert & Roberta) and make predictions using them.\n",
        "Non-Deep Learning Models: https://colab.research.google.com/drive/1YA74vX3MzxYZezwQSOMSwBK8byLi6Qr1#scrollTo=P8SeGgmbaRJ-\n",
        "\n",
        "  Deep Learning Models: https://colab.research.google.com/drive/1w8NaUk-D4PYi9loTI3yBtG-si-eJ6YRU#scrollTo=-1TjJiBS9Q3s\n",
        "\n",
        "9. **Analysis of Data and Conclusion**: https://colab.research.google.com/drive/1B0XHgVp65mlze1MIY4PZe0stlZ5LsfqQ#scrollTo=LFkG5F6hDlLZ\n",
        "\n",
        "10. **Developement of the application having an user interface and backend connected to the prediction by the models**: https://colab.research.google.com/drive/1Hr56vo57RRJ6mmtICzrcFaXKpppQ7hll\n",
        " (Running this code will generate an url that be be opened to launch the application)"
      ],
      "metadata": {
        "id": "c-bPgWwJZjTM"
      }
    }
  ]
}